{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1589766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import zipfile\n",
    "import os\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0be59a",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Opening the zip file\n",
    "# import zipfile\n",
    "\n",
    "# zip_path = \"/Users/lex/CodeProjects/MyProject/Mitaron/Data/Child Medical Expense Dataset Oct 2025.zip\"\n",
    "# password = b\"keio20251028\"\n",
    "\n",
    "# with zipfile.ZipFile(zip_path) as zf:\n",
    "#     zf.extractall(pwd=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸€ã¤ã®csvã‚’parquetã«\n",
    "from pathlib import Path\n",
    "\n",
    "def csv_to_parquet(csv_path, parquet_folder = \"/Users/lex/CodeProjects/MyProject/Mitaron/Data/parquete_data\"):\n",
    "    \"\"\"\n",
    "    Converting a CSV file to parquet and saving it to folder\n",
    "\n",
    "    Args:\n",
    "        file_path (_type_): path to an initial .csv file\n",
    "        parquet_folder: folder where Parquet files will be saved\n",
    "\n",
    "    Returns: \n",
    "        -Polars DataFrameÂ¥\n",
    "    \"\"\"\n",
    "    csv_path = Path(csv_path)\n",
    "    parquet_folder = Path(parquet_folder)\n",
    "\n",
    "    print(f\"Reading: {csv_path}\")\n",
    "    df = pl.read_csv(csv_path, encoding=\"cp932\")\n",
    "\n",
    "    # Creating parquet file name\n",
    "    parquet_file = parquet_folder / csv_path.with_suffix('.parquet').name\n",
    "\n",
    "    # Saving as parquet\n",
    "    df.write_parquet(parquet_file)\n",
    "    print(f\"âœ… Saved: {parquet_file}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e9d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼å…¨ä½“ã‚’CSV -> parquet\n",
    "def convert_folder_to_parquet_with_progress(csv_folder, parquet_folder=\"/Users/lex/CodeProjects/MyProject/Mitaron/Data/parquete_data\"):\n",
    "    \"\"\"\n",
    "    Convert all CSV files in a folder to Parquet format with progress bar.\n",
    "    \n",
    "    Args:\n",
    "        csv_folder: folder containing CSV files\n",
    "        parquet_folder: folder where Parquet files will be saved\n",
    "    \n",
    "    Returns:\n",
    "        - Dictionary with success/error counts and file lists\n",
    "    \"\"\"\n",
    "    csv_dir = Path(csv_folder)\n",
    "    parquet_dir = Path(parquet_folder)\n",
    "    \n",
    "    # Create parquet folder if it doesn't exist\n",
    "    parquet_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files\n",
    "    csv_files = list(csv_dir.glob(\"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"âš ï¸ No CSV files found in {csv_folder}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to convert\\n\")\n",
    "    \n",
    "    success_files = []\n",
    "    error_files = []\n",
    "    \n",
    "    # Process with progress bar\n",
    "    for csv_file in tqdm(csv_files, desc=\"Converting CSV to Parquet\"):\n",
    "        try:\n",
    "            df = pl.read_csv(csv_file, encoding=\"cp932\")\n",
    "            parquet_file = parquet_dir / csv_file.with_suffix('.parquet').name\n",
    "            df.write_parquet(parquet_file)\n",
    "            success_files.append(csv_file.name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"âŒ Error: {csv_file.name} - {e}\")\n",
    "            error_files.append(csv_file.name)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Conversion Complete!\")\n",
    "    print(f\"âœ… Success: {len(success_files)} files\")\n",
    "    print(f\"âŒ Errors: {len(error_files)} files\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if error_files:\n",
    "        print(\"\\nFiles with errors:\")\n",
    "        for f in error_files:\n",
    "            print(f\"  - {f}\")\n",
    "    \n",
    "    return {\n",
    "        \"success_count\": len(success_files),\n",
    "        \"error_count\": len(error_files),\n",
    "        \"success_files\": success_files,\n",
    "        \"error_files\": error_files\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a621e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files to convert\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting CSV to Parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:16<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Conversion Complete!\n",
      "âœ… Success: 8 files\n",
      "âŒ Errors: 0 files\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'success_count': 8,\n",
       " 'error_count': 0,\n",
       " 'success_files': ['dataset_000001_050000.csv',\n",
       "  'dataset_100001_150000.csv',\n",
       "  'dataset_200001_250000.csv',\n",
       "  'dataset_275001_300000.csv',\n",
       "  'dataset_150001_200000.csv',\n",
       "  'dataset_250001_275000.csv',\n",
       "  'dataset_050001_100000.csv',\n",
       "  'dataset_300001_312573.csv'],\n",
       " 'error_files': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_folder_to_parquet_with_progress(\"JAST_raw/dataset\", parquet_folder=\"/Users/lex/CodeProjects/MyProject/Mitaron/Data/parquete_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e918f05",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0064c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for counting the unique number of variable in the folder:\n",
    "def count_unique_infolder(folder_path, column):\n",
    "    \"\"\"Counting the Unique variables across the folders _\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): path of the folder you want to find, make sure it is the parquet folder \n",
    "        column (str): the column through which you want to go\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Table with filename and unique count\n",
    "    \"\"\"\n",
    "    parquet_dir = Path(folder_path)\n",
    "    parquet_files = list(parquet_dir.glob(\"*.parquet\"))\n",
    "\n",
    "    if not parquet_files:\n",
    "        print(f\"âš ï¸ No parquet files found in {parquet_files}\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    for parquet in tqdm(parquet_files, desc=\"Checking Parquet\"):\n",
    "        try:\n",
    "            df = pl.read_parquet(parquet)\n",
    "            if column not in df.columns:\n",
    "                tqdm.write(f\"âš ï¸ Column {column} not found in {parquet.name}\")\n",
    "                continue\n",
    "            n_unique = df[column].n_unique()\n",
    "            results.append({\"file\": parquet.name, \"n_unique\": n_unique})\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"âŒ Error reading {parquet.name}: {e}\")\n",
    "\n",
    "    # Combining Results\n",
    "    if not results:\n",
    "        print(\"No results found\")\n",
    "        return None\n",
    "    \n",
    "    result_df = pl.DataFrame(results).sort(\"n_unique\")\n",
    "\n",
    "    print(\"\\nâœ… Unique value counts by file:\")\n",
    "    print(result_df)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "550578b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_areaid_infolder(folder_path, column=\"area_id\"):\n",
    "    \"\"\"\n",
    "    Counts how many occurrences of each area_id appear in each parquet file in a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing parquet files.\n",
    "        column (str): Column name to count (default: \"area_id\").\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Table with file, area_id, and count.\n",
    "    \"\"\"\n",
    "    parquet_dir = Path(folder_path)\n",
    "    parquet_files = list(parquet_dir.glob(\"*.parquet\"))\n",
    "\n",
    "    if not parquet_files:\n",
    "        print(f\"âš ï¸ No parquet files found in {folder_path}\")\n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for parquet in tqdm(parquet_files, desc=\"Counting area_id occurrences\"):\n",
    "        try:\n",
    "            # Read each parquet file (eager for simplicity)\n",
    "            df = pl.read_parquet(parquet)\n",
    "            if column not in df.columns:\n",
    "                tqdm.write(f\"âš ï¸ Column {column} not found in {parquet.name}\")\n",
    "                continue\n",
    "\n",
    "            # Count occurrences of each area_id\n",
    "            counts = (\n",
    "                df.group_by(column)\n",
    "                  .agg(pl.count().alias(\"count\"))\n",
    "                  .with_columns(pl.lit(parquet.name).alias(\"file\"))\n",
    "            )\n",
    "\n",
    "            # Append to results list\n",
    "            results.append(counts)\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"âŒ Error reading {parquet.name}: {e}\")\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    if not results:\n",
    "        print(\"No valid results found.\")\n",
    "        return None\n",
    "\n",
    "    result_df = pl.concat(results)\n",
    "\n",
    "    # Optional: sort results by file then area_id\n",
    "    result_df = result_df.sort([\"file\", column])\n",
    "\n",
    "    print(\"\\nâœ… Area ID counts by file:\")\n",
    "    print(result_df)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf01f679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking Parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 20.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Unique value counts by file:\n",
      "shape: (8, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ file                          â”† n_unique â”‚\n",
      "â”‚ ---                           â”† ---      â”‚\n",
      "â”‚ str                           â”† i64      â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ dataset_300001_312573.parquet â”† 12535    â”‚\n",
      "â”‚ dataset_275001_300000.parquet â”† 24965    â”‚\n",
      "â”‚ dataset_250001_275000.parquet â”† 24976    â”‚\n",
      "â”‚ dataset_200001_250000.parquet â”† 49802    â”‚\n",
      "â”‚ dataset_000001_050000.parquet â”† 49868    â”‚\n",
      "â”‚ dataset_050001_100000.parquet â”† 49888    â”‚\n",
      "â”‚ dataset_100001_150000.parquet â”† 49896    â”‚\n",
      "â”‚ dataset_150001_200000.parquet â”† 49929    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Determining folder to use ###\n",
    "folder_path = \"/Users/lex/CodeProjects/MyProject/Mitaron/Data/parquete_data\"\n",
    "column = \"patient_id\"\n",
    "###\n",
    "result_df = count_unique_infolder(folder_path, column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73ea01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311859\n"
     ]
    }
   ],
   "source": [
    "print(result_df[\"n_unique\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346758e9",
   "metadata": {},
   "source": [
    "### Fixing the Code names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import polars as pl\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def process_parquet_folder(input_folder: str, output_folder: str):\n",
    "#     \"\"\"\n",
    "#     Converts multiple `area_` dummy columns into a single `area_id` column\n",
    "#     for all .parquet files in a folder and saves them to a new folder.\n",
    "\n",
    "#     Args:\n",
    "#         input_folder (str): Path to the folder containing .parquet files.\n",
    "#         output_folder (str): Path to the folder to save transformed files.\n",
    "#     \"\"\"\n",
    "#     input_dir = Path(input_folder)\n",
    "#     output_dir = Path(output_folder)\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     parquet_files = list(input_dir.glob(\"*.parquet\"))\n",
    "#     if not parquet_files:\n",
    "#         print(f\"âš ï¸ No parquet files found in {input_folder}\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Found {len(parquet_files)} parquet files to process.\\n\")\n",
    "\n",
    "#     for file_path in tqdm(parquet_files, desc=\"Processing Parquet Files\"):\n",
    "#         try:\n",
    "#             # Lazy scan\n",
    "#             lf = pl.scan_parquet(file_path)\n",
    "\n",
    "#             # Identify area columns\n",
    "#             area_cols = [c for c in lf.columns if c.startswith(\"area_\")]\n",
    "\n",
    "#             if not area_cols:\n",
    "#                 print(f\"âš ï¸ No area_ columns found in {file_path.name}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Unpivot (wide â†’ long)\n",
    "#             lf_long = lf.unpivot(\n",
    "#                 index=[c for c in lf.columns if c not in area_cols],\n",
    "#                 on=area_cols,\n",
    "#                 variable_name=\"area\",\n",
    "#                 value_name=\"visit_number\"\n",
    "#             )\n",
    "\n",
    "#             # **CRITICAL: Filter to keep only rows where visit number > 0**\n",
    "#             lf_long = lf_long.filter(pl.col(\"visit_number\") >= 1)\n",
    "\n",
    "#             # Create numeric area_id\n",
    "#             lf_long = lf_long.with_columns(\n",
    "#                 pl.col(\"area\")\n",
    "#                 .str.replace(\"area_\", \"\")\n",
    "#                 .cast(pl.Int32)  # Convert to integer\n",
    "#                 .alias(\"area_id\")\n",
    "#             )\n",
    "\n",
    "#             # Drop leftover columns\n",
    "#             lf_result = lf_long.drop([\"area\"])\n",
    "\n",
    "#             # Move area_id to 12th column if possible\n",
    "#             cols = lf_result.columns\n",
    "#             if \"area_id\" in cols:\n",
    "#                 cols.remove(\"area_id\")\n",
    "#                 insert_pos = min(11, len(cols))\n",
    "#                 cols.insert(insert_pos, \"area_id\")\n",
    "#                 lf_result = lf_result.select(cols)\n",
    "\n",
    "#             # Save to output folder (same filename)\n",
    "#             output_file = output_dir / file_path.name\n",
    "#             lf_result.sink_parquet(output_file)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"\\nâŒ Error processing {file_path.name}: {str(e)}\")\n",
    "#             continue\n",
    "\n",
    "#     print(f\"\\nâœ… All files processed and saved to: {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b05b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f568733d",
   "metadata": {},
   "source": [
    "# Newest Version of the Column Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "06ffe379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_parquet_folder(input_folder: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    Converts multiple `area_` columns into a single `area_id` column (primary/most visited area)\n",
    "    for all .parquet files in a folder and saves them to a new folder.\n",
    "    Also deletes specified unnecessary columns.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing .parquet files.\n",
    "        output_folder (str): Path to the folder to save transformed files.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_folder)\n",
    "    output_dir = Path(output_folder)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    parquet_files = list(input_dir.glob(\"*.parquet\"))\n",
    "    if not parquet_files:\n",
    "        print(f\"âš ï¸ No parquet files found in {input_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(parquet_files)} parquet files to process.\\n\")\n",
    "\n",
    "    # Columns to delete\n",
    "    deleting_cols = [\n",
    "        \"qualification_date\", \"disqualification_date\", \"shibou_flg\",\n",
    "        \"bed_cnt_rank_0\", \"bed_cnt_rank_1_19\", \"bed_cnt_rank_20_199\", \"bed_cnt_rank_200\"\n",
    "    ]\n",
    "\n",
    "    for file_path in tqdm(parquet_files, desc=\"Processing Parquet Files\"):\n",
    "        try:\n",
    "            # Lazy scan\n",
    "            lf = pl.scan_parquet(file_path)\n",
    "\n",
    "            # Identify area columns\n",
    "            area_cols = [c for c in lf.columns if c.startswith(\"area_\")]\n",
    "\n",
    "            if not area_cols:\n",
    "                print(f\"âš ï¸ No area_ columns found in {file_path.name}\")\n",
    "                continue\n",
    "\n",
    "            # Get identifying columns (non-area columns)\n",
    "            id_cols = [c for c in lf.columns if c not in area_cols]\n",
    "\n",
    "            # Unpivot (wide â†’ long)\n",
    "            lf_long = lf.unpivot(\n",
    "                index=id_cols,\n",
    "                on=area_cols,\n",
    "                variable_name=\"area\",\n",
    "                value_name=\"visit_number\"\n",
    "            )\n",
    "\n",
    "            # Filter to keep only rows where visit number > 0\n",
    "            lf_long = lf_long.filter(pl.col(\"visit_number\") > 0)\n",
    "\n",
    "            # Create numeric area_id\n",
    "            lf_long = lf_long.with_columns(\n",
    "                pl.col(\"area\")\n",
    "                .str.replace(\"area_\", \"\")\n",
    "                .cast(pl.Int32)\n",
    "                .alias(\"area_id\")\n",
    "            )\n",
    "\n",
    "            # Keep only the area with maximum visits per original row\n",
    "            # In case of ties, keep the one with lowest area_id (tiebreaker)\n",
    "            lf_result = (\n",
    "                lf_long\n",
    "                .sort([\"visit_number\", \"area_id\"], descending=[True, False])\n",
    "                .group_by(id_cols)\n",
    "                .first()\n",
    "            )\n",
    "\n",
    "            # Drop leftover columns\n",
    "            lf_result = lf_result.drop([\"area\"])\n",
    "\n",
    "            # ğŸ”¹ Drop unnecessary columns if they exist\n",
    "            lf_result = lf_result.drop([c for c in deleting_cols if c in lf_result.columns])\n",
    "\n",
    "            # Move area_id and visit_number next to each other (starting at 12th col if possible)\n",
    "            cols = lf_result.columns\n",
    "            if \"area_id\" in cols and \"visit_number\" in cols:\n",
    "                cols.remove(\"area_id\")\n",
    "                cols.remove(\"visit_number\")\n",
    "                insert_pos = min(11, len(cols))\n",
    "                cols.insert(insert_pos, \"area_id\")\n",
    "                cols.insert(insert_pos + 1, \"visit_number\")\n",
    "                lf_result = lf_result.select(cols)\n",
    "\n",
    "            # Save to output folder (same filename)\n",
    "            output_file = output_dir / file_path.name\n",
    "            lf_result.sink_parquet(output_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Error processing {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nâœ… All files processed and saved to: {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a4b6918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 parquet files to process.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Parquet Files:   0%|          | 0/8 [00:00<?, ?it/s]/var/folders/p_/j1bs4y9n44n99d120x17dn_00000gn/T/ipykernel_46122/4017328061.py:38: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  area_cols = [c for c in lf.columns if c.startswith(\"area_\")]\n",
      "/var/folders/p_/j1bs4y9n44n99d120x17dn_00000gn/T/ipykernel_46122/4017328061.py:45: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  id_cols = [c for c in lf.columns if c not in area_cols]\n",
      "/var/folders/p_/j1bs4y9n44n99d120x17dn_00000gn/T/ipykernel_46122/4017328061.py:79: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  lf_result = lf_result.drop([c for c in deleting_cols if c in lf_result.columns])\n",
      "/var/folders/p_/j1bs4y9n44n99d120x17dn_00000gn/T/ipykernel_46122/4017328061.py:82: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  cols = lf_result.columns\n",
      "Processing Parquet Files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:28<00:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All files processed and saved to: /Users/lex/CodeProjects/MyProject/Mitaron/Parquet_fresh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"/Users/lex/CodeProjects/MyProject/Mitaron/Data/parquete_data\"\n",
    "output_folder = \"/Users/lex/CodeProjects/MyProject/Mitaron/Parquet_fresh\"\n",
    "process_parquet_folder(input_folder,output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "318c0460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(792873, 42) (792873, 79)\n"
     ]
    }
   ],
   "source": [
    "df_origin = pl.read_parquet(\"/Users/lex/CodeProjects/MyProject/Mitaron/Data/parquete_data/dataset_050001_100000.parquet\")\n",
    "df_new = pl.read_parquet(\"/Users/lex/CodeProjects/MyProject/Mitaron/Parquet_fresh/dataset_050001_100000.parquet\")\n",
    "print(df_new.shape, df_origin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "537904d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (792_873, 42)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>patient_no</th><th>insurer_cd</th><th>patient_id</th><th>sex_type_nm</th><th>birth_date</th><th>medtreat_yymm</th><th>rezept_family_type_nm</th><th>annual_salary_rank</th><th>business_type</th><th>public_expense_cd</th><th>ika_in_rz_cnt</th><th>area_id</th><th>ika_in_medtreat_days</th><th>ika_in_req_amt</th><th>ika_out_rz_cnt</th><th>ika_out_medtreat_days</th><th>ika_out_req_amt</th><th>sika_rz_cnt</th><th>sika_medtreat_days</th><th>sika_req_amt</th><th>cho_rz_cnt</th><th>cho_medtreat_days</th><th>cho_req_amt</th><th>sick_cd_cnt</th><th>sick_cd_main_cnt</th><th>sick_cd_utagai_cnt</th><th>outcome_type_tiyu</th><th>iy_amt</th><th>iy_std_cd7_cnt</th><th>yj_cd_cnt</th><th>iy_times_max</th><th>ge_rate</th><th>iy_min_amt_gap</th><th>iy_refill_times_1</th><th>iy_refill_times_2</th><th>iy_refill_times_3</th><th>ika_si_cd_cnt</th><th>sika_si_cd_cnt</th><th>sika_si_kasan_cnt</th><th>cho_si_cd_cnt</th><th></th><th>visit_number</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i32</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>78320</td><td>89</td><td>&quot;RI0007849804&quot;</td><td>&quot;ç”·&quot;</td><td>201807</td><td>202002</td><td>&quot;å®¶æ—&quot;</td><td>&quot;750ï½799ä¸‡å††&quot;</td><td>&quot;åŒ–å­¦å·¥æ¥­ãƒ»åŒé¡ä¼¼æ¥­&quot;</td><td>0</td><td>0</td><td>141003</td><td>0</td><td>0</td><td>2</td><td>2</td><td>11570</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>7</td><td>0</td><td>0</td><td>0</td><td>null</td><td>2</td></tr><tr><td>78377</td><td>89</td><td>&quot;RI0007970296&quot;</td><td>&quot;å¥³&quot;</td><td>201810</td><td>202211</td><td>&quot;å®¶æ—&quot;</td><td>&quot;750ï½799ä¸‡å††&quot;</td><td>&quot;åŒ–å­¦å·¥æ¥­ãƒ»åŒé¡ä¼¼æ¥­&quot;</td><td>0</td><td>0</td><td>141003</td><td>0</td><td>0</td><td>2</td><td>3</td><td>20020</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4</td><td>0</td><td>0</td><td>1</td><td>30</td><td>2</td><td>2</td><td>1</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>7</td><td>0</td><td>0</td><td>0</td><td>null</td><td>2</td></tr><tr><td>78343</td><td>89</td><td>&quot;RI0007912597&quot;</td><td>&quot;ç”·&quot;</td><td>201201</td><td>201808</td><td>&quot;å®¶æ—&quot;</td><td>&quot;750ï½799ä¸‡å††&quot;</td><td>&quot;åŒ–å­¦å·¥æ¥­ãƒ»åŒé¡ä¼¼æ¥­&quot;</td><td>0</td><td>0</td><td>141003</td><td>0</td><td>0</td><td>2</td><td>2</td><td>13930</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>2</td><td>0</td><td>0</td><td>120</td><td>1</td><td>1</td><td>3</td><td>&quot;0.0000&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>11</td><td>0</td><td>0</td><td>0</td><td>null</td><td>2</td></tr><tr><td>76963</td><td>89</td><td>&quot;RI0005070802&quot;</td><td>&quot;ç”·&quot;</td><td>201512</td><td>201810</td><td>&quot;å®¶æ—&quot;</td><td>&quot;750ï½799ä¸‡å††&quot;</td><td>&quot;åŒ–å­¦å·¥æ¥­ãƒ»åŒé¡ä¼¼æ¥­&quot;</td><td>1</td><td>1</td><td>141003</td><td>3</td><td>380160</td><td>1</td><td>1</td><td>16650</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>1</td><td>1</td><td>1</td><td>6000</td><td>10</td><td>12</td><td>1</td><td>&quot;0.0000&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>53</td><td>0</td><td>0</td><td>0</td><td>null</td><td>2</td></tr><tr><td>65981</td><td>77</td><td>&quot;RI0003592469&quot;</td><td>&quot;ç”·&quot;</td><td>200910</td><td>201801</td><td>&quot;å®¶æ—&quot;</td><td>&quot;750ï½799ä¸‡å††&quot;</td><td>&quot;ãã®ä»–&quot;</td><td>2</td><td>1</td><td>141003</td><td>8</td><td>408870</td><td>1</td><td>2</td><td>6140</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>22</td><td>1</td><td>4</td><td>1</td><td>81190</td><td>21</td><td>22</td><td>10</td><td>&quot;0.2777&quot;</td><td>870</td><td>0</td><td>0</td><td>0</td><td>77</td><td>0</td><td>0</td><td>0</td><td>null</td><td>2</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>90214</td><td>101</td><td>&quot;RI0006501615&quot;</td><td>&quot;å¥³&quot;</td><td>200201</td><td>201801</td><td>&quot;å®¶æ—&quot;</td><td>&quot;750ï½799ä¸‡å††&quot;</td><td>&quot;ãã®ä»–&quot;</td><td>0</td><td>0</td><td>342076</td><td>0</td><td>0</td><td>1</td><td>1</td><td>3290</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1400</td><td>2</td><td>2</td><td>28</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>7</td><td>0</td><td>0</td><td>0</td><td>null</td><td>1</td></tr><tr><td>79791</td><td>91</td><td>&quot;RI0000429173&quot;</td><td>&quot;å¥³&quot;</td><td>200903</td><td>202108</td><td>&quot;å®¶æ—&quot;</td><td>&quot;1250ä¸‡å††ä»¥ä¸Š&quot;</td><td>&quot;é‡‘èæ¥­ã€ä¿é™ºæ¥­&quot;</td><td>0</td><td>0</td><td>342076</td><td>0</td><td>0</td><td>1</td><td>1</td><td>2820</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>7</td><td>0</td><td>0</td><td>0</td><td>null</td><td>1</td></tr><tr><td>65094</td><td>77</td><td>&quot;RI0002047620&quot;</td><td>&quot;å¥³&quot;</td><td>201703</td><td>201909</td><td>&quot;å®¶æ—&quot;</td><td>&quot;750ï½799ä¸‡å††&quot;</td><td>&quot;ãã®ä»–&quot;</td><td>1</td><td>0</td><td>342076</td><td>0</td><td>0</td><td>1</td><td>5</td><td>24650</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>12</td><td>3</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>null</td><td>1</td></tr><tr><td>89632</td><td>101</td><td>&quot;RI0004165850&quot;</td><td>&quot;ç”·&quot;</td><td>200301</td><td>201909</td><td>&quot;å®¶æ—&quot;</td><td>&quot;750ï½799ä¸‡å††&quot;</td><td>&quot;ãã®ä»–&quot;</td><td>0</td><td>0</td><td>342076</td><td>0</td><td>0</td><td>1</td><td>9</td><td>57080</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>1</td><td>0</td><td>0</td><td>4920</td><td>7</td><td>7</td><td>1</td><td>&quot;0.0000&quot;</td><td>200</td><td>0</td><td>0</td><td>0</td><td>11</td><td>0</td><td>0</td><td>0</td><td>null</td><td>1</td></tr><tr><td>65968</td><td>77</td><td>&quot;RI0003573921&quot;</td><td>&quot;ç”·&quot;</td><td>202007</td><td>202108</td><td>&quot;å®¶æ—&quot;</td><td>&quot;750ï½799ä¸‡å††&quot;</td><td>&quot;ãã®ä»–&quot;</td><td>1</td><td>0</td><td>342076</td><td>0</td><td>0</td><td>1</td><td>1</td><td>2170</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4</td><td>0</td><td>0</td><td>0</td><td>null</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (792_873, 42)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ patient_no â”† insurer_cd â”† patient_id â”† sex_type_n â”† â€¦ â”† sika_si_k â”† cho_si_cd â”†      â”† visit_num â”‚\n",
       "â”‚ ---        â”† ---        â”† ---        â”† m          â”†   â”† asan_cnt  â”† _cnt      â”† ---  â”† ber       â”‚\n",
       "â”‚ i64        â”† i64        â”† str        â”† ---        â”†   â”† ---       â”† ---       â”† str  â”† ---       â”‚\n",
       "â”‚            â”†            â”†            â”† str        â”†   â”† i64       â”† i64       â”†      â”† i64       â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 78320      â”† 89         â”† RI00078498 â”† ç”·         â”† â€¦ â”† 0         â”† 0         â”† null â”† 2         â”‚\n",
       "â”‚            â”†            â”† 04         â”†            â”†   â”†           â”†           â”†      â”†           â”‚\n",
       "â”‚ 78377      â”† 89         â”† RI00079702 â”† å¥³         â”† â€¦ â”† 0         â”† 0         â”† null â”† 2         â”‚\n",
       "â”‚            â”†            â”† 96         â”†            â”†   â”†           â”†           â”†      â”†           â”‚\n",
       "â”‚ 78343      â”† 89         â”† RI00079125 â”† ç”·         â”† â€¦ â”† 0         â”† 0         â”† null â”† 2         â”‚\n",
       "â”‚            â”†            â”† 97         â”†            â”†   â”†           â”†           â”†      â”†           â”‚\n",
       "â”‚ 76963      â”† 89         â”† RI00050708 â”† ç”·         â”† â€¦ â”† 0         â”† 0         â”† null â”† 2         â”‚\n",
       "â”‚            â”†            â”† 02         â”†            â”†   â”†           â”†           â”†      â”†           â”‚\n",
       "â”‚ 65981      â”† 77         â”† RI00035924 â”† ç”·         â”† â€¦ â”† 0         â”† 0         â”† null â”† 2         â”‚\n",
       "â”‚            â”†            â”† 69         â”†            â”†   â”†           â”†           â”†      â”†           â”‚\n",
       "â”‚ â€¦          â”† â€¦          â”† â€¦          â”† â€¦          â”† â€¦ â”† â€¦         â”† â€¦         â”† â€¦    â”† â€¦         â”‚\n",
       "â”‚ 90214      â”† 101        â”† RI00065016 â”† å¥³         â”† â€¦ â”† 0         â”† 0         â”† null â”† 1         â”‚\n",
       "â”‚            â”†            â”† 15         â”†            â”†   â”†           â”†           â”†      â”†           â”‚\n",
       "â”‚ 79791      â”† 91         â”† RI00004291 â”† å¥³         â”† â€¦ â”† 0         â”† 0         â”† null â”† 1         â”‚\n",
       "â”‚            â”†            â”† 73         â”†            â”†   â”†           â”†           â”†      â”†           â”‚\n",
       "â”‚ 65094      â”† 77         â”† RI00020476 â”† å¥³         â”† â€¦ â”† 0         â”† 0         â”† null â”† 1         â”‚\n",
       "â”‚            â”†            â”† 20         â”†            â”†   â”†           â”†           â”†      â”†           â”‚\n",
       "â”‚ 89632      â”† 101        â”† RI00041658 â”† ç”·         â”† â€¦ â”† 0         â”† 0         â”† null â”† 1         â”‚\n",
       "â”‚            â”†            â”† 50         â”†            â”†   â”†           â”†           â”†      â”†           â”‚\n",
       "â”‚ 65968      â”† 77         â”† RI00035739 â”† ç”·         â”† â€¦ â”† 0         â”† 0         â”† null â”† 1         â”‚\n",
       "â”‚            â”†            â”† 21         â”†            â”†   â”†           â”†           â”†      â”†           â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c462005",
   "metadata": {},
   "outputs": [],
   "source": [
    "deleting_cols = [\"qualification_date\", \"disqualification_date\", \"shibou_flg\", \"bed_cnt_rank_0\",\"bed_cnt_rank_1_19\", \"bed_cnt_rank_20_199\", \"bed_cnt_rank_200\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScienceEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
